#分布式锁

------------------------------------------------

为什么要用?

    1.效率 
    避免重复计算/重复消耗资源 比如重复发送邮件
    可以容忍出现错误
    2.正确性
    在任何情况下都不允许锁失效的情况发生 因为一旦发生
    就可能意味着数据不一致(inconsistency) 数据丢失 文件损坏 或者其它严重的问题。

单机版Redis分布式锁

    SET resource_name my_random_value NX PX 30000

    NX表示不存在才创建
    PX表示过期时间

    my_random_value是由客户端生成的一个随机字符串
    它要保证在足够长的一段时间内在所有客户端的所有获取锁的请求中都是唯一的

    释放锁:(lua脚本)
    if redis.call("get",KEYS[1]) == ARGV[1] then
    return redis.call("del",KEYS[1])
    else
    return 0
    end

    为什么要有随机字符串?
    答:考虑以下情况
    客户端1获取锁成功。
    客户端1在某个操作上阻塞了很长时间。
    过期时间到了，锁自动释放了。
    客户端2获取到了对应同一个资源的锁。
    客户端1从阻塞中恢复过来，释放掉了客户端2持有的锁。
    可见可能会释放掉别的客户端持有的锁

    有什么问题?
    Redis节点宕机, 主从复制（replication）是异步的
    这可能导致在failover过程中丧失锁的安全性


RedLock

    1.获取当前时间（毫秒数）

    2.按顺序依次向N个Redis节点(master)执行获取锁的操作
    这个获取操作跟前面基于单Redis节点的获取锁的过程相同 包含随机字符串my_random_value 
    也包含过期时间(比如PX 30000，即锁的有效时间) 为了保证在某个Redis节点不可用的时候算法能够继续运行 
    这个获取锁的操作还有一个超时时间(time out) 它要远小于锁的有效时间（几十毫秒量级）客户端在向某个Redis节点获取锁失败以后
    应该立即尝试下一个Redis节点  这里的失败 应该包含任何类型的失败 比如该Redis节点不可用   或者该Redis节点上的锁已经被其它客户端持有
    （注：Redlock原文中这里只提到了Redis节点不可用的情况，但也应该包含其它的失败情况）

    3.计算整个获取锁的过程总共消耗了多长时间 计算方法是用当前时间减去第1步记录的时间 如果客户端从大多数Redis节点（>= N/2+1）成功获取到了锁
    并且获取锁总共消耗的时间没有超过锁的有效时间(lock validity time) 那么这时客户端才认为最终获取锁成功；否则，认为最终获取锁失败。
    为什么要这么做?
    答:考虑一系列延迟,可能反馈到达客户端的时候,锁已经失效了,没有算这个消耗时间的话,客户端无法知道是否失效了已经.

    4.如果最终获取锁成功了，那么这个锁的有效时间应该重新计算，它等于最初的锁的有效时间减去第3步计算出来的获取锁消耗的时间。

    5.如果最终获取锁失败了（可能由于获取到锁的Redis节点个数少于N/2+1，或者整个获取锁的过程消耗的时间超过了锁的最初有效时间）
    那么客户端应该立即向所有Redis节点发起释放锁的操作（即上面的Redis Lua脚本） 
    为什么没成功的也要发松解锁呢?
    答:客户端发给某个Redis节点的获取锁的请求成功到达了该Redis节点，这个节点也成功执行了SET操作，
    但是它返回给客户端的响应包却丢失了。这在客户端看来，获取锁的请求由于超时而失败了，但在Redis这边看来，加锁已经成功了。

    问题?
    1.Redis节点崩溃重启
    假设一共有5个Redis节点：A, B, C, D, E。设想发生了如下的事件序列：
    客户端1成功锁住了A, B, C，获取锁成功（但D和E没有锁住）。
    节点C崩溃重启了，但客户端1在C上加的锁没有持久化下来，丢失了。
    节点C重启后，客户端2锁住了C, D, E，获取锁成功。
    解决:
    Redis的AOF持久化方式是每秒写一次磁盘（即执行fsync） ,为了尽可能不丢数据 Redis允许设置成每次修改数据都进行fsync 但这会降低性能
    延迟重启:一个节点崩溃后 先不立即重启它 而是等待一段时间再重启 这段时间应该大于锁的有效时间(lock validity time)。
    这样的话这个节点在重启前所参与的锁都会过期 它在重启后就不会对现有的锁造成影响。

    2.第三步之后的时延
    虽然拿到了锁,但是请求资源/修改的资源的过程中,可能锁失效了,同时另外的客户端获得了锁,这样就出现了多个客户端同时访问资源的情况


基于数据库的分布式锁

    1.利用insert进行插入
    2.利用事务中的select for update 行锁进行操作

    方案一中,
    `id` INT (11) NOT NULL AUTO_INCREMENT COMMENT '主键',
	`resource_name` VARCHAR (64) NOT NULL DEFAULT '' COMMENT '资源名', //唯一索引
    问题:
    数据库挂掉/失效需要客户端主动操作/非排队,获取失败直接返回错误/非重入

    方案二中,
    for update语句会在执行成功后立即返回，在执行失败时一直处于阻塞状态，直到成功。
    锁定之后服务宕机，无法释放？使用这种方式，服务宕机之后数据库会自己把锁释放掉。
    可重入和单点故障问题未解决

基于Zookeeper的分布式锁

    实现1.
    利用的zk的临时节点, 在争抢锁的时候,所有的客户端都尝试创建一个临时节点(代表锁住的资源),只有一个客户端会创建成功,
    创建成功的客户端得到锁,其它的客户端则监听(利用zk的watch)该节点的状态改变并且进入阻塞,节点改变后 zk server 
    会通知剩下的客户端,剩下的客户端停止阻塞并且重新争抢锁.

    缺点:
    节点的删除都会通知所有的客户端,并且所有的客户端会 取消监听 + 重新一起争夺锁 + 争夺失败 + 再次开启监听 ,如此循环,
    资源耗费多,并且这种耗费是可以避免的,那么如何避免呢?就是下面第二种的 改进版分布式锁.

    实现2.
    这一种分布式锁的实现是利用zk的临时顺序节点,每一个客户端在争夺锁的时候都由zk分配一个顺序号(sequence),
    客户端则按照这个顺序去获取锁.
    不过lockPath(锁住的资源)是一个持久节点,客户端在该持久节点下面创建临时顺序节点,获取到顺序号后,
    根据自己是否是最小的顺序号来获取锁,顺序号最小则获取锁,序号不为最小则监听(watch)前一个顺序号,
    当前一个顺序号被删除的时候表明锁被释放了,则会通知下一个客户端.

    注意:
    watch是一次性的 需要重新监听

    问题?
    1.
    客户端1创建了znode节点/lock，获得了锁。
    客户端1进入了长时间的GC pause。
    客户端1连接到ZooKeeper的Session过期了。znode节点/lock被自动删除。
    客户端2创建了znode节点/lock，从而获得了锁。
    客户端1从GC pause中恢复过来，它仍然认为自己持有锁。
    和RedLock一样,有延迟的问题
    
    和Redis比较?
    1.Redis需要自己设置超时时间,这个时间不好把握
    Z基于session 灵活
    2.基于ZooKeeper的锁支持在获取锁失败之后等待锁重新释放的事件
    这让客户端对锁的使用更加灵活
    
怎么解决共有的这种延迟问题?

    首先延迟的具体定义:
     一个进程持有锁L，发起了请求R，但是请求失败了。另一个进程获得了锁L并在请求R到达目的方之前执行了一些动作。
    如果后来请求R到达了，它就有可能在没有锁L保护的情况下进行操作，带来数据不一致的潜在风险

    方案1:
    fencing token机制本质上是要求客户端在每次访问一个共享资源的时候，在执行任何操作之前，
    先对资源进行某种形式的“标记”(mark)操作，这个“标记”能保证持有旧的锁的客户端请求（如果延迟到达了）
    无法操作资源。这种标记操作可以是很多形式，fencing token是其中比较典型的一个

Google的Chubby

    锁的持有者可以随时请求一个sequencer，这是一个字节串，它由三部分组成：
    锁的名字。
    锁的获取模式（排他锁还是共享锁）。
    lock generation number（一个64bit的单调递增数字）。作用相当于fencing token或epoch number。

    两种方案:
    1.客户端拿到sequencer之后，在操作资源的时候把它传给资源服务器。然后，资源服务器负责对sequencer的有效性进行检查。检查可以有两种方式：
    调用Chubby提供的API，CheckSequencer()，将整个sequencer传进去进行检查。这个检查是为了保证客户端持有的锁在进行资源访问的时候仍然有效。

    2.将客户端传来的sequencer与资源服务器当前观察到的最新的sequencer进行对比检查。可以理解为与Martin描述的对于fencing token的检查类似。

    3.由于兼容的原因 资源服务本身不容易修改 那么Chubby还提供了一种机制：
    lock-delay。
    Chubby允许客户端为持有的锁指定一个lock-delay的时间值（默认是1分钟）。(类似于tcp连接中的close_wait)
    当Chubby发现客户端被动失去联系的时候，并不会立即释放锁，而是会在lock-delay指定的时间内阻止其它客户端获得这个锁。
    这是为了在把锁分配给新的客户端之前，让之前持有锁的客户端有充分的时间把请求队列排空(draining the queue)，
    尽量防止出现延迟到达的未处理请求。